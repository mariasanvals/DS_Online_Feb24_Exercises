{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "from openai import ChatCompletion, Completion, OpenAI\n",
    "from pypdf import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\edici\\Nigmae\\Sherezade\\Brain\\Building_Sharzade\\Labs\\aiclose.oai\",\"r\") as f:\n",
    "    apikey = json.load(f)[\"token\"]\n",
    "openai.api_key = apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo-0125\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consulta(rol,prompt):\n",
    "    respuesta =  client.chat.completions.create(\n",
    "                model= MODEL,\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": rol},\n",
    "                    {\"role\": \"user\",\"content\": prompt}\n",
    "                ],\n",
    "                temperature = 0,\n",
    "    )\n",
    "    return respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key= apikey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS_05_21_02_D02.pdf\n",
      "43\n",
      "DS_05_21_02_D03.pdf\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "respuestas = []\n",
    "pages_to_include = 5\n",
    "for fichero in os.listdir(\"data\"):\n",
    "    print(fichero)\n",
    "    reader = PdfReader(\"data/\" + fichero)\n",
    "    number_of_pages = len(reader.pages)\n",
    "    text = []\n",
    "    for page in reader.pages[0:pages_to_include]:\n",
    "        text.append(page.extract_text())\n",
    "    print(number_of_pages)\n",
    "    rol = \"Eres un experto en modelos LLM y en inteligencia artificial\"\n",
    "    prompt = f'''Hazme un resumen del texto que te incluyo entre triples comillas \"\"\"{text}\"\"\". Por favor NO quiero el resumen en formato texto, lo quiero en markdown con las palabras más importantes en negrita e incluye el título en inglés y español'''\n",
    "    respuestas.append(consulta(rol, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "respuesta = respuestas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.types.chat.chat_completion.ChatCompletion"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Large Language Models: A Survey / Modelos de Lenguaje Grande: Una Encuesta\n",
      "\n",
      "**Abstract**  \n",
      "- **LLMs** have drawn a lot of attention due to their strong performance on a wide range of natural language tasks.\n",
      "- **LLMs**' ability of general-purpose language understanding and generation is acquired by training billions of model’s parameters on massive amounts of text data.\n",
      "- The research area of **LLMs** is evolving rapidly in many different ways.\n",
      "- This paper reviews some of the most prominent **LLMs**, including three popular **LLM** families (GPT, LLaMA, PaLM), and discusses their characteristics, contributions, and limitations.\n",
      "- It also gives an overview of techniques developed to build and augment **LLMs**.\n",
      "- The paper surveys popular datasets prepared for **LLM** training, fine-tuning, and evaluation, reviews widely used **LLM** evaluation metrics, and compares the performance of several popular **LLMs** on a set of representative benchmarks.\n",
      "- Finally, it concludes by discussing open challenges and future research directions.\n",
      "\n",
      "**I. INTRODUCTION / INTRODUCCIÓN**  \n",
      "- Language modeling is a long-standing research topic, dating back to the 1950s with Shannon’s application of information theory to human language.\n",
      "- Statistical language modeling became fundamental to many natural language understanding and generation tasks.\n",
      "- The recent advances on transformer-based large language models (**LLMs**), pretrained on Web-scale text corpora, significantly extended the capabilities of language models.\n",
      "- **LLMs** are becoming the basic building block for the development of general-purpose AI agents or artificial general intelligence (**AGI**).\n",
      "\n",
      "**II. LARGE LANGUAGE MODELS / MODELOS DE LENGUAJE GRANDE**  \n",
      "- The section reviews early pre-trained neural language models as the base of **LLMs**, and then focuses on three families of **LLMs**: GPT, LlaMA, and PaLM.\n",
      "\n",
      "**III. HOW LLMS ARE BUILT / CÓMO SE CONSTRUYEN LOS LLM**  \n",
      "- Discusses data cleaning, large language model families, dominant **LLM** architectures, tokenizations, positional encoding, model pre-training, fine-tuning, and instruction tuning.\n",
      "\n",
      "**IV. HOW LLMS ARE USED AND AUGMENTED / CÓMO SE UTILIZAN Y SE AUMENTAN LOS LLM**  \n",
      "- Discusses **LLM** limitations, cost-effective training/inference, adaptation & compression, using **LLMs** for prompt design and engineering, augmenting **LLMs** through external knowledge, and using external tools.\n",
      "\n",
      "**V. POPULAR DATASETS FOR LLMS / CONJUNTOS DE DATOS POPULARES PARA LLM**  \n",
      "- Presents datasets for basic tasks: language modeling/understanding/generation, emergent tasks like in-context learning, reasoning, instruction following, and augmented tasks using external knowledge/tools.\n",
      "\n",
      "**VI. PROMINENT LLMS’ PERFORMANCE ON BENCHMARKS / RENDIMIENTO DE LOS PRINCIPALES LLM EN BENCHMARKS**  \n",
      "- Reviews the performance of popular **LLMs** on benchmarks.\n",
      "\n",
      "**VII. CHALLENGES AND FUTURE DIRECTIONS / DESAFÍOS Y DIRECCIONES FUTURAS**  \n",
      "- Concludes the paper by summarizing the challenges and future research directions.\n",
      "\n",
      "### Paper Structure / Estructura del Documento\n",
      "\n",
      "- Provides a high-level overview of popular language models, including BERT, RoBERTa, ALBERT, XLNet, GPT-1, GPT-2, T5, MT5, BART, GPT-3, GPT-4, and other models in the LLaMA and PaLM families.\n"
     ]
    }
   ],
   "source": [
    "print(respuesta.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DS_05_21_02_D02.pdf\n",
    "43\n",
    "DS_05_21_02_D03.pdf\n",
    "41\n",
    "openai.types.chat.chat_completion.ChatCompletion\n",
    "### Large Language Models: A Survey / Modelos de Lenguaje Grande: Una Encuesta\n",
    "\n",
    "**Abstract**  \n",
    "- **LLMs** have drawn a lot of attention due to their strong performance on a wide range of natural language tasks.\n",
    "- **LLMs**' ability of general-purpose language understanding and generation is acquired by training billions of model’s parameters on massive amounts of text data.\n",
    "- The research area of **LLMs** is evolving rapidly in many different ways.\n",
    "- This paper reviews some of the most prominent **LLMs**, including three popular **LLM** families (GPT, LLaMA, PaLM), and discusses their characteristics, contributions, and limitations.\n",
    "- It also gives an overview of techniques developed to build and augment **LLMs**.\n",
    "- The paper surveys popular datasets prepared for **LLM** training, fine-tuning, and evaluation, reviews widely used **LLM** evaluation metrics, and compares the performance of several popular **LLMs** on a set of representative benchmarks.\n",
    "- Finally, it concludes by discussing open challenges and future research directions.\n",
    "\n",
    "**I. INTRODUCTION / INTRODUCCIÓN**  \n",
    "- Language modeling is a long-standing research topic, dating back to the 1950s with Shannon’s application of information theory to human language.\n",
    "- Statistical language modeling became fundamental to many natural language understanding and generation tasks.\n",
    "- The recent advances on transformer-based large language models (**LLMs**), pretrained on Web-scale text corpora, significantly extended the capabilities of language models.\n",
    "- **LLMs** are becoming the basic building block for the development of general-purpose AI agents or artificial general intelligence (**AGI**).\n",
    "\n",
    "**II. LARGE LANGUAGE MODELS / MODELOS DE LENGUAJE GRANDE**  \n",
    "- The section reviews early pre-trained neural language models as the base of **LLMs**, and then focuses on three families of **LLMs**: GPT, LlaMA, and PaLM.\n",
    "\n",
    "**III. HOW LLMS ARE BUILT / CÓMO SE CONSTRUYEN LOS LLM**  \n",
    "- Discusses data cleaning, large language model families, dominant **LLM** architectures, tokenizations, positional encoding, model pre-training, fine-tuning, and instruction tuning.\n",
    "\n",
    "**IV. HOW LLMS ARE USED AND AUGMENTED / CÓMO SE UTILIZAN Y SE AUMENTAN LOS LLM**  \n",
    "- Discusses **LLM** limitations, cost-effective training/inference, adaptation & compression, using **LLMs** for prompt design and engineering, augmenting **LLMs** through external knowledge, and using external tools.\n",
    "\n",
    "**V. POPULAR DATASETS FOR LLMS / CONJUNTOS DE DATOS POPULARES PARA LLM**  \n",
    "- Presents datasets for basic tasks: language modeling/understanding/generation, emergent tasks like in-context learning, reasoning, instruction following, and augmented tasks using external knowledge/tools.\n",
    "\n",
    "**VI. PROMINENT LLMS’ PERFORMANCE ON BENCHMARKS / RENDIMIENTO DE LOS PRINCIPALES LLM EN BENCHMARKS**  \n",
    "- Reviews the performance of popular **LLMs** on benchmarks.\n",
    "\n",
    "**VII. CHALLENGES AND FUTURE DIRECTIONS / DESAFÍOS Y DIRECCIONES FUTURAS**  \n",
    "- Concludes the paper by summarizing the challenges and future research directions.\n",
    "\n",
    "### Paper Structure / Estructura del Documento\n",
    "\n",
    "- Provides a high-level overview of popular language models, including BERT, RoBERTa, ALBERT, XLNet, GPT-1, GPT-2, T5, MT5, BART, GPT-3, GPT-4, and other models in the LLaMA and PaLM families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```markdown\n",
      "### A Comprehensive Overview of Large Language Models\n",
      "### Una Visión Integral de los Modelos de Lenguaje Grandes\n",
      "\n",
      "- **Autores:** Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian\n",
      "- **Afiliaciones:** Universidades en Pakistán, China, Australia, y Arabia Saudita\n",
      "- **Resumen:** Los Modelos de Lenguaje Grandes (LLMs) han demostrado capacidades notables en tareas de procesamiento de lenguaje natural y más allá. Este artículo proporciona una visión general de la literatura existente sobre una amplia gama de conceptos relacionados con LLMs, desde innovaciones arquitectónicas hasta estrategias de entrenamiento, mejoras en la longitud del contexto, ajuste fino, LLMs multimodales, robótica, conjuntos de datos, evaluación, eficiencia, y más.\n",
      "- **Términos Clave:** Modelos de Lenguaje Grandes, LLMs, chatGPT, LLMs Aumentados, LLMs Multimodales, Entrenamiento de LLMs, Evaluación de LLMs\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(respuestas[1].choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A Comprehensive Overview of Large Language Models\n",
    "### Una Visión Integral de los Modelos de Lenguaje Grandes\n",
    "\n",
    "- **Autores:** Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, Ajmal Mian\n",
    "- **Afiliaciones:** Universidades en Pakistán, China, Australia, y Arabia Saudita\n",
    "- **Resumen:** Los Modelos de Lenguaje Grandes (LLMs) han demostrado capacidades notables en tareas de procesamiento de lenguaje natural y más allá. Este artículo proporciona una visión general de la literatura existente sobre una amplia gama de conceptos relacionados con LLMs, desde innovaciones arquitectónicas hasta estrategias de entrenamiento, mejoras en la longitud del contexto, ajuste fino, LLMs multimodales, robótica, conjuntos de datos, evaluación, eficiencia, y más.\n",
    "- **Términos Clave:** Modelos de Lenguaje Grandes, LLMs, chatGPT, LLMs Aumentados, LLMs Multimodales, Entrenamiento de LLMs, Evaluación de LLMs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias:\n",
    "\n",
    "[La librería para trabajar con los LLM programáticamente: LangChain](https://www.langchain.com/langchain)  \n",
    "[Para saber más: Andrew Ng y deeplearning.ai](https://www.deeplearning.ai/short-courses/)  \n",
    "[Lo que está de moda: RAG (Retrieval Augmented Generation) y alguna cosilla más](https://datos.gob.es/en/blog/rag-retrieval-augmented-generation-key-unlocks-door-precision-language-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Curso_Online_DATA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
